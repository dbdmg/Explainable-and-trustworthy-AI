{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Lab 5 Concept-based Explainable AI**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "RmXrdF8Ix4fv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Teaching assistant**: Eleonora Poeta ( eleonora.poeta@polito.it )\n",
    "\n",
    "**Lab 5**: Concept-based XAI - CRAFT"
   ],
   "metadata": {
    "id": "L8ZJeZ5Epxk4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **CRAFT**"
   ],
   "metadata": {
    "id": "NWC7Dzib5jHN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "[CRAFT](https://arxiv.org/pdf/2211.10154) is a novel **post-hoc concept-based method**.\n",
    "\n",
    "It does the **automated extraction** of high-level **concepts** that neural networks have learned.\n",
    "\n",
    "It has the following key attributes:\n",
    "\n",
    "* **Multi-Layer Concept Extraction**: It allows for the extraction of concepts from *various locations within the model*, thus enabling the identification of the **most pertinent laye**r for representing individual concepts.\n",
    "*  **Concept Importance Assessment**: CRAFT computes the **significance** of **individual** **concepts** concerning the model's predictions through the use of Sobol indices.\n",
    "* **Concept Attribution Map**: It allows to backpropagate concept scores into the pixel space, leading to \"concept attribution heatmaps\" generation."
   ],
   "metadata": {
    "id": "s_y9lFrd5n0L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Quick Overview of CRAFT Approach**\n",
    "\n",
    "A CRAFT investigation involves the following steps:\n",
    "\n",
    "1. **Input images selection**: Select a set of images ***X*** from the dataset where the model's predictions align with the class under investigation.\n",
    "\n",
    "2. **Layer Selection**: Choose a layer to initiate the investigation, and split the model into 2 parts at this location: the 1st part ***g*** computes the activations of our input images, while the 2nd part ***h*** computes the logits.<br> **CRAFT will extract concepts from the activation space of this selected layer.**\n",
    "\n",
    "3. **CRAFT Fitting**:\n",
    " * **Crops Extraction**: CRAFT automatically **extracts random image crops** from the input dataset. This choice is motivated by the expectation that concepts are present in these crops, and can be subsequently dissected. CRAFT will operate on these crops to factorize the concepts. These crops are also used to visualize the concepts.\n",
    "\n",
    " * **Concept Activation Factorization**: In this phase, we use the 1st model ***g*** to compute the random crops activations, and then apply a Non-negative Matrix Factorization (NMF) to decompose these positive activations into two matrices:\n",
    "  >* W constitutes a dictionary of Concept Activation Vectors (CAVs). It can be understood as a \"concept basis\" or \"concept bank\",\n",
    "  >* U represents the concept values, which are coefficients allowing to redefine the data points in our dataset according to the concept basis\n",
    "\n",
    "5. **Concept Importance Estimation**: **Sensitivity** analysis is employed to estimate the global importance of each concept across the entire dataset.\n",
    "\n",
    "  * **In general**, the **Sensitivity** measures the degree to which a model's output or predictions are influenced by changes or perturbations in specific features or concepts.\n",
    "  * CRAFT uses Sobol indices to measure the contribution of each concept on the model's output.\n",
    "  * Once this step is achieved, it is possible to compute the **contribution of each concept for a specific image**, thus capturing its local importance. CRAFT provides a variety of plotting functions to showcase the concepts and their respective importance.\n",
    "\n",
    "6. Visualization of **Concept Map Attribution** : CRAFT has the ability to generate concept-wise attribution maps. These maps display the **part of an image that triggered the detection of the concept** by the network."
   ],
   "metadata": {
    "id": "1I89AT38rlk8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **Exercise 1**\n",
    "\n"
   ],
   "metadata": {
    "id": "aGTr__1evZ0F"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To implement CRAFT you have to refer to **[Xplique](https://github.com/deel-ai/xplique)** library available on Github.\n",
    "\n",
    "This library is composed of different modules:\n",
    "  * The *Concepts* module allows you to extract human concepts from a model and to test their usefulness with respect to a class."
   ],
   "metadata": {
    "id": "-_i15KSrwlvf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Steps** to follow:\n",
    "\n",
    "\n",
    "\n",
    "0.   Select the **GPU Runtime**.\n",
    "1.   Install the **Xplique library** running the given commands.\n",
    "2.   **Download the data**. You will download the class of rabbits. Run the given command\n",
    "3. **CRAFT** requires splitting the model in two parts. So, you have to prepare  two functions $(g, h)$ such that $f(x) = (g \\cdot h)(x)$.\n",
    "\n",
    "  * **$g$** is the function that maps our input to the latent space (the penultimate layer of our model).\n",
    "  > * As **$g$** you will use the **ResNet50** from `torchvision.models\n",
    "  > * **$g$** will be `'input_to_latent'` part. We need to take the *first 8 layers* of the ResNet50.\n",
    "\n",
    "  * **$h$** is the function that maps from the layer before the classification head to the output.\n",
    "   > * **$h$** will be `'latent_to_logit'` part.\n",
    "   > You will implement this classifier head (follow the instruction in the comments of the cell)\n",
    "\n",
    "4. Define **transformation for the image** data from `torchvision.transforms`. You have to:\n",
    "  * Transform the np array to a **PIL Image**\n",
    "  * **Resize** the image up to 256\n",
    "  * **CentreCrop** it to 256\n",
    "  * Trasform the PIL Image to a **tensor**\n",
    "  * **Normalize** the image with the given values\n",
    "\n",
    "5. **Instanciate CRAFT**\n",
    "6. **Fit** CRAFT\n",
    "7. Calculate (global) **concept importances**\n",
    "8. **Plot** some visualizations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> N.B: Check the documentation at this [link](https://deel-ai.github.io/xplique/latest/api/concepts/craft/) .\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "-aL8rW3far4E"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution:"
   ],
   "metadata": {
    "id": "mndetP-P7fcN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Imports"
   ],
   "metadata": {
    "id": "qBAlp95ox1qS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Install Xplique\n",
    "!pip install -q xplique"
   ],
   "metadata": {
    "id": "q9x5fT-95nXw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Get some rabbit images\n",
    "!curl -O https://share.deel.ai/s/fq78w4E2GTrQ54S/download && mv download rabbit.npz"
   ],
   "metadata": {
    "id": "6Cqn3Fc70IQt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "Qy1xh21F1fR9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check the device you are using is cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "id": "LvK2TwfZcQhW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load a ResNet model from torchvision.models\n",
    "# Complete with your code\n"
   ],
   "metadata": {
    "id": "tS3tUrB2cV_o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cut the model in two part, g and h\n",
    "\n",
    "# g is our 'input_to_latent'. For ResNet18 you have to take the first 8 layers\n",
    "g = nn.Sequential(*(list(model.children())[:8]))"
   ],
   "metadata": {
    "id": "9XYkYHkVdnIn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(g)"
   ],
   "metadata": {
    "id": "_ugjvG5XoZvu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# h is our 'latent_to_logit'\n",
    "# Complete with your code\n",
    "\n",
    "h = nn.Sequential(\n",
    "    , # AdaptiveAvgPool2d(1,1)\n",
    "    , # Flatten\n",
    "    , # Dropout with p=0.0\n",
    "    , # Linear with in_features=2048, out_features=1000\n",
    "    , # Identity\n",
    "  )\n",
    "\n",
    "\n",
    "h = h.to(device)"
   ],
   "metadata": {
    "id": "2rVyp7tF0Db2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(h)"
   ],
   "metadata": {
    "id": "CC8yli8948yP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define tranformation for our image data\n",
    "# Complete with your code !\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "transf = transforms.Compose([\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "])"
   ],
   "metadata": {
    "id": "Yn5B2Pa4f551"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "rabbit_class = 330 # imagenet class for rabbit\n",
    "\n",
    "# loading some images of rabbits !\n",
    "images = np.load('rabbit.npz')['arr_0'].astype(np.uint8)\n",
    "\n",
    "images_preprocessed = torch.stack([transf(img) for img in images], 0)\n",
    "\n",
    "images_preprocessed = images_preprocessed.to(device)\n",
    "\n",
    "images_preprocessed.shape"
   ],
   "metadata": {
    "id": "a9tlPmFqfQYR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fitting CRAFT: Patches Creation and Concepts Factorization"
   ],
   "metadata": {
    "id": "NEWcWkf0hip2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from xplique.concepts import CraftTorch as Craft\n",
    "from xplique.concepts import DisplayImportancesOrder\n",
    "\n",
    "\n",
    "# Complete with your code !\n",
    "\n",
    "# Instanciate CRAFT\n",
    "craft = Craft(input_to_latent_model = .....,\n",
    "              latent_to_logit_model = ....,\n",
    "              number_of_concepts = 10,\n",
    "              patch_size = 80,\n",
    "              batch_size = 64,\n",
    "              device = device)"
   ],
   "metadata": {
    "id": "nSNAGO1YfQuZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# now we can start fit the concept using our rabbit images\n",
    "# CRAFT will:\n",
    "# 1. Create the patches\n",
    "# 2. Find the concepts\n",
    "# 3. Return the crops (crops), the embedding of the crops (crops_u), and the concept bank (concept_bank_w)\n",
    "crops, crops_u, concept_bank_w = craft.fit(, # images preprocessed\n",
    "                                           class_id=) # id of the class of rabbit\n",
    "\n",
    "crops.shape, crops_u.shape, concept_bank_w.shape"
   ],
   "metadata": {
    "id": "8YT70YO7hlFV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate the concept importances with Sobol score. Use the `estimate_importance()` function."
   ],
   "metadata": {
    "id": "-uoMaLJoIMB8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "importances = craft.estimate_importance()"
   ],
   "metadata": {
    "id": "tY9tE5uWhrmt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the attribution map for 1 image, and the concepts associated to the class around it, ordered by global importance\n",
    "# Select one of the images_processed (e.g. 21 ) and put it on .cpu()\n",
    "\n",
    "# Complete with your code!\n",
    "\n",
    "craft.plot_image_concepts(.....)"
   ],
   "metadata": {
    "id": "IcxT5MQ8jC1l"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's see which concepts matter\n",
    "\n",
    "# Plot the importance values. Use the plot_concepts_importances()\n",
    "# Complete with your code !\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "CIPx9w1HjHRv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's inspect those concepts by showing the 10 best crops for each concept.\n",
    "\n",
    "# Limit the display to the 5 most important concepts.\n",
    "craft.plot_concepts_crops(nb_crops=,\n",
    "                          nb_most_important_concepts=)"
   ],
   "metadata": {
    "id": "6urzBwQejJS6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Display the attribution map for the first 20 images. Remeber to put them on .cpu()\n",
    "\n",
    "# Limit the display to the 5 most important concepts.\n",
    "craft.plot_concept_attribution_maps(.... ,\n",
    "                                    nb_most_important_concepts=)"
   ],
   "metadata": {
    "id": "JU3iRs00pkCa"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
